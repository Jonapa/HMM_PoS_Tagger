{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, init_matrix, transition_matrix, emission_matrix, word_to_index, category_to_index, index_to_category):\n",
    "    num_categories = len(init_matrix) # num categories N\n",
    "    num_words = len(sentence) # num words in the sentence T\n",
    "\n",
    "    # viterbi table\n",
    "    viterbi_table = np.full((num_categories, num_words), -np.inf) # inf for logarithms\n",
    "    backpointer_table = np.zeros((num_categories, num_words), dtype=int)\n",
    "\n",
    "\n",
    "    # first step (init)\n",
    "    for category in range(num_categories):\n",
    "        word_index = word_to_index.get(sentence[0], word_to_index[\"UNK\"]) # we use UNK tag if the word is not in the vocab\n",
    "        viterbi_table[category, 0] = init_matrix[category] + emission_matrix[category, word_index] # sum because of logs\n",
    "        \n",
    "\n",
    "    # recursion step\n",
    "    for t in range (1, num_words): # from 2 to T\n",
    "        for category in range(num_categories): # from 1 to N\n",
    "            max_prob = -np.inf\n",
    "            max_prev_category = 0\n",
    "            word_index = word_to_index.get(sentence[t], word_to_index[\"UNK\"])\n",
    "\n",
    "            for prev_category in range(num_categories):\n",
    "                prev_viterbi = viterbi_table[prev_category, t-1]\n",
    "                transition_score = transition_matrix[prev_category, category]\n",
    "                '''\n",
    "                if word_index is not None:\n",
    "                    emission_score = emission_matrix[category, word_index]\n",
    "                else:\n",
    "                    emission_score = 0\n",
    "                '''\n",
    "                emission_score = emission_matrix[category, word_index]\n",
    "                current_prob = prev_viterbi + transition_score + emission_score\n",
    "\n",
    "                if current_prob > max_prob:\n",
    "                    max_prob = current_prob\n",
    "                    max_prev_category = prev_category\n",
    "\n",
    "            # update tables viterbi and backpointer\n",
    "            viterbi_table[category, t] = max_prob\n",
    "            backpointer_table[category, t] = max_prev_category # a la que apunta \n",
    "\n",
    "    # final probability step\n",
    "    max_final_category = np.argmax(viterbi_table[:, -1]) # all the rows of the last column\n",
    "    max_prob = viterbi_table[max_final_category, -1] # maximum probability is the one in the last column of the max_final category row\n",
    "\n",
    "    # construct optimal sequence\n",
    "    optimal_seq = [max_final_category]\n",
    "    for t in range(num_words-1, 0, -1): # from num_words-1 until 1, backward\n",
    "        back_value = backpointer_table[optimal_seq[0], t]\n",
    "        optimal_seq.insert(0, back_value)\n",
    "\n",
    "    optimal_tags = [index_to_category[idx] for idx in optimal_seq]\n",
    "    \n",
    "    return optimal_tags, max_prob\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
