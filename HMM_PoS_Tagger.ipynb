{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf9ULXkPYWRz"
   },
   "source": [
    "# HMM PoS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqAfMXv6Ycew"
   },
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Georgetown University Multilayer Corpus (GUM) is an open source multilayer corpus of richly annotated texts from 16 text types.**\n",
    "\n",
    "- https://gucorpling.org/gum/\n",
    "\n",
    "- Zeldes, Amir (2017) \"The GUM Corpus: Creating Multilayer Resources in the Classroom\". Language Resources and Evaluation 51(3), 581–612. https://link.springer.com/article/10.1007/s10579-016-9343-x\n",
    "\n",
    "- https://github.com/UniversalDependencies/UD_English-GUM/blob/master/README.md\n",
    "\n",
    "\n",
    "**A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation (GENTLE) is a manually annotated multilayer corpus following the same design and annotation layers as GUM, but of unusual text types.**\n",
    "\n",
    "- https://gucorpling.org/gum/gentle.html\n",
    "\n",
    "- Aoyama, Tatsuya, Behzad, Shabnam, Gessler, Luke, Levine, Lauren, Lin, Jessica, Liu, Yang Janet, Peng, Siyao, Zhu, Yilun and Zeldes, Amir (2023) \"GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation\". In: Proceedings of the Seventeenth Linguistic Annotation Workshop (LAW-XVII 2023). Toronto, Canada. https://arxiv.org/abs/2306.01966\n",
    "\n",
    "- https://github.com/UniversalDependencies/UD_English-GENTLE/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 17 unique PoS Tags in the datasets. In the following code chunk the number of tokens per tag is shown followed by the most common lemmas. However, GUM's stats file presents some incosistences regarding the number of tokens per tag (as our algorithm recovers correctly these numbers for the GENTLE case, but are some minimal differences with GUM, we believe the stats file of the repo is wrong). \n",
    "\n",
    "It is worth mentioning that exists a _ tag that should be avoided like the following example:\n",
    "\n",
    "- 12-13\tworkforce’s\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "\n",
    "- 12\tworkforce\tworkforce\tNOUN\tNN\tNumber=Sing\t14\tnmod:poss\t14:nmod:poss\t_\n",
    "\n",
    "- 13\t’s\t's\tPART\tPOS\t_\t12\tcase\t12:case\tEntity=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" https://github.com/UniversalDependencies/UD_English-GUM/blob/master/stats.xml\n",
    "<!-- Statistics of universal POS tags. The comments show the most frequent lemmas. -->\n",
    "<tags unique=\"17\">\n",
    "<tag name=\"ADJ\">13961</tag><!-- good, other, first, new, many, great, little, large, more, different -->\n",
    "<tag name=\"ADP\">20170</tag><!-- of, in, to, for, on, with, at, from, by, as -->\n",
    "<tag name=\"ADV\">10103</tag><!-- so, when, just, then, also, how, now, more, here, really -->\n",
    "<tag name=\"AUX\">11355</tag><!-- be, have, do, can, will, would, should, could, may, might -->\n",
    "<tag name=\"CCONJ\">7057</tag><!-- and, or, but, both, &, either, nor, yet, neither, plus -->\n",
    "<tag name=\"DET\">17331</tag><!-- the, a, this, all, that, some, no, any, every, another -->\n",
    "<tag name=\"INTJ\">2023</tag><!-- like, yeah, oh, well, so, um, uh, no, okay, yes -->\n",
    "<tag name=\"NOUN\">35507</tag><!-- person, time, year, day, thing, way, life, city, world, work -->\n",
    "<tag name=\"NUM\">3994</tag><!-- one, two, 1, three, 2, 3, four, 4, five, 10 -->\n",
    "<tag name=\"PART\">5113</tag><!-- to, not, 's -->\n",
    "<tag name=\"PRON\">17819</tag><!-- I, it, you, we, that, they, he, his, this, your -->\n",
    "<tag name=\"PROPN\">12184</tag><!-- State, President, University, America, York, New, Warhol, Figure, American, south -->\n",
    "<tag name=\"PUNCT\">28955</tag><!-- ,, ., '', -, ?, (, ), —, [, : -->\n",
    "<tag name=\"SCONJ\">3393</tag><!-- that, if, as, because, for, of, by, while, in, after -->\n",
    "<tag name=\"SYM\">317</tag><!-- -, /, $, %, +, =, DKK, €, £, § -->\n",
    "<tag name=\"VERB\">22277</tag><!-- have, know, go, make, do, get, say, be, take, think -->\n",
    "<tag name=\"X\">361</tag><!-- _, et, al., de, 1, 1., 2., in, situ, 2 -->\n",
    "</tags>\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" https://github.com/UniversalDependencies/UD_English-GENTLE/blob/master/stats.xml\n",
    "<!-- Statistics of universal POS tags. The comments show the most frequent lemmas. -->\n",
    "<tags unique=\"17\">\n",
    "<tag name=\"ADJ\">1240</tag><!-- next, other, first, old, open, more, good, straight, chronic, right -->\n",
    "<tag name=\"ADP\">1588</tag><!-- of, in, to, for, with, on, from, by, at, as -->\n",
    "<tag name=\"ADV\">729</tag><!-- then, just, so, well, here, also, thus, how, where, now -->\n",
    "<tag name=\"AUX\">753</tag><!-- be, will, have, can, do, would, may, should, could, shall -->\n",
    "<tag name=\"CCONJ\">618</tag><!-- and, or, but, &, either, /, plus, yet, both, neither -->\n",
    "<tag name=\"DET\">1195</tag><!-- the, a, this, all, no, any, that, some, each, every -->\n",
    "<tag name=\"INTJ\">60</tag><!-- fucking, please, ah, well, oh, okay, so, uh, ha, now -->\n",
    "<tag name=\"NOUN\">3783</tag><!-- week, x, T, project, school, S, person, day, time, y -->\n",
    "<tag name=\"NUM\">386</tag><!-- one, 1, 5, 2, two, 4, 3, X, 10, three -->\n",
    "<tag name=\"PART\">349</tag><!-- to, not, 's -->\n",
    "<tag name=\"PRON\">1188</tag><!-- I, you, he, it, we, his, that, my, your, they -->\n",
    "<tag name=\"PROPN\">901</tag><!-- Company, JavaScript, Book, Proposition, Court, English, Week, Career, React, Agreement -->\n",
    "<tag name=\"PUNCT\">2655</tag><!-- ,, ., :, '', (, ), -, ;, —, ! -->\n",
    "<tag name=\"SCONJ\">234</tag><!-- that, if, as, because, in, by, like, of, while, before -->\n",
    "<tag name=\"SYM\">167</tag><!-- ⪯, ∈, =, -, ⋅, /, %, +, $, ≤ -->\n",
    "<tag name=\"VERB\">1653</tag><!-- have, go, do, get, take, see, follow, make, know, let -->\n",
    "<tag name=\"X\">300</tag><!-- 1., 2., 3., 4., 5., 6., 7., 8., 9., 10. -->\n",
    "</tags>\n",
    "\"\"\"\n",
    "\n",
    "PoS_tags = [\n",
    "    (\"ADJ\", 12348, 1621, 1240),\n",
    "    (\"ADP\", 17702, 2481, 1588),\n",
    "    (\"ADV\", 8989, 1115, 729),\n",
    "    (\"AUX\", 10174, 1189, 753),\n",
    "    (\"CCONJ\", 6218, 839, 618),\n",
    "    (\"DET\", 15224, 2111, 1195),\n",
    "    (\"INTJ\", 1859, 164, 60),\n",
    "    (\"NOUN\", 31274, 4240, 3783),\n",
    "    (\"NUM\", 3554, 440, 386),\n",
    "    (\"PART\", 4595, 519, 349),\n",
    "    (\"PRON\", 16077, 1746, 1188),\n",
    "    (\"PROPN\", 10557, 1627, 901),\n",
    "    (\"PUNCT\", 25928, 3027, 2655),\n",
    "    (\"SCONJ\", 3053, 340, 234),\n",
    "    (\"SYM\", 282, 35, 167),\n",
    "    (\"VERB\", 19865, 2479, 1653),\n",
    "    (\"X\", 329, 32, 300),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents come from a wide variety of sources. In order to group them in an efficient way, those documents belonging to same area are gathered together. \n",
    "\n",
    "For instance, GUM_academic_art, GUM_academic_census... will belong to GUM_academic. \n",
    "\n",
    "In the following code block, each distinction is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/UniversalDependencies/UD_English-GUM/tree/master/not-to-release/sources\n",
    "\n",
    "doc_type_GUM = [\n",
    "    \"GUM_academic\",\n",
    "    \"GUM_bio\",\n",
    "    \"GUM_conversation\",\n",
    "    \"GUM_court\",\n",
    "    \"GUM_essay\",\n",
    "    \"GUM_fiction\",\n",
    "    \"GUM_interview\",\n",
    "    \"GUM_letter\",\n",
    "    \"GUM_news\",\n",
    "    \"GUM_podcast\",\n",
    "    \"GUM_speech\",\n",
    "    \"GUM_textbook\",\n",
    "    \"GUM_vlog\",\n",
    "    \"GUM_voyage\",\n",
    "    \"GUM_whow\",\n",
    "]\n",
    "\n",
    "# https://github.com/UniversalDependencies/UD_English-GENTLE/tree/master/not-to-release/sources\n",
    "\n",
    "doc_type_GENTLE = [\n",
    "    \"GENTLE_dictionary\",\n",
    "    \"GENTLE_esports\",\n",
    "    \"GENTLE_legal\",\n",
    "    \"GENTLE_medical\",\n",
    "    \"GENTLE_poetry\",\n",
    "    \"GENTLE_proof\",\n",
    "    \"GENTLE_syllabus\",\n",
    "    \"GENTLE_threat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4ff3_FeYfhJ"
   },
   "source": [
    "## HMM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FOR COLAB\n",
    "!pip install conllu\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install requests\n",
    "!pip install bs4\n",
    "!pip install nltk\n",
    "\n",
    "!gdown \"1tml8gZm9ZnANjETv6_hUg46N-fRemIAa&confirm=t\"\n",
    "!unzip data.zip\n",
    "!rm -rf data.zip\n",
    "'''\n",
    "\n",
    "%pip install conllu\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install requests\n",
    "%pip install bs4\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from conllu import parse_incr\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the HMM model, we will follow these steps:\n",
    "\n",
    "- Initialize the model with three parameters:\n",
    "    path_data: The directory where all necessary data is stored.\n",
    "    lemmatize: A flag indicating whether the model should lemmatize the tokens.\n",
    "    threshold: A percentage threshold that determines which words are kept in the vocabulary. Remaining words will be replaced with the UNK token.\n",
    "\n",
    "- Prepare the training and testing datasets and verify that the number of tokens per document is correct. For each partition, the data will be stored in a Dict[str, List[List[Tuple[str, str]]]] structure, in which for each document type (key of the Dict) each sentence is a list of tuples in the form (word/lemma, PoS tag).\n",
    "\n",
    "- To create the vocabulary for the model, we will use only the training dataset to calculate the frequency of each word/lemma. These words/lemmas will be sorted in descending order by frequency. Using the previously set threshold, we will determine the cutoff point for the vocabulary percentage. All words/lemmas above this threshold will be retained in the vocabulary, while the rest will be discarded (and, therefore, considered as UNK). \n",
    "\n",
    "- Finally, the emission and transition matrices of the Hidden Markov Model will be calculated. To do this, the following matrices will be computed using the training dataset and then smoothed using Laplace smoothing. Moreover, log probabilities will be used to prevent underflow.\n",
    "\n",
    "$$ \n",
    "\n",
    "\\sum_{i=1}^{n} \\log \\left( \\frac{\\text{count}(y_{i-1}, y_i) + 1}{\\text{count}(y_{i-1}) + |Y|} \\right) \\quad \\text{(Transition)}\n",
    "\n",
    "\n",
    "\n",
    "+ \\sum_{i=1}^{n} \\log \\left( \\frac{\\text{count}(y_i, x_i) + 1}{\\text{count}(y_i) + |X|} \\right) \\quad \\text{(Emission) }\n",
    "\n",
    "\\\\\n",
    "\\text{where } \\quad y \\text{ is the PoS tag} \\quad x \\text{ is the word/lemma, } \\\\\n",
    "\\quad |X| \\text{ is the size of the vocabulary, and } \\quad |Y| \\text{ is the number of tags.}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "igkBhNAXnt23"
   },
   "outputs": [],
   "source": [
    "# Implementation of the HMM model\n",
    "class HMM_PoS_tagger:\n",
    "    def __init__(self, path_data: str, lemmatize: bool, threshold: float):\n",
    "        self.path_data = path_data\n",
    "        self.lemmatize = lemmatize\n",
    "        self.threshold = threshold\n",
    "        self.counter = Counter()  # 12770 unique tokens in train, 188028 total tokens\n",
    "        # Read data from train and test datasets & check that is correct\n",
    "        self.train = self.read_train_data()\n",
    "        self.test_GUM = self.read_test_data(is_GUM=True)\n",
    "        self.test_GENTLE = self.read_test_data(is_GUM=False)\n",
    "        # Create vocabulary\n",
    "        self.vocab = self.create_vocab()\n",
    "        # Add UNK token to the vocabulary\n",
    "        self.vocab = [\"UNK\"] + self.vocab\n",
    "        # Add START and STOP tags to the tags\n",
    "        self.tags = [\"START\", \"STOP\"] + [x[0] for x in PoS_tags]\n",
    "        # Train the model and get the emission and transition matrices\n",
    "        self.emission, self.transition = self.train_model()\n",
    "\n",
    "    def read_train_data(self) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "\n",
    "        # Train and Dev datasets will be used to train the model\n",
    "        paths = [\n",
    "            os.path.join(self.path_data, \"en_gum-ud-train.conllu\"),\n",
    "            os.path.join(self.path_data, \"en_gum-ud-dev.conllu\"),\n",
    "        ]\n",
    "\n",
    "        data = self.read_data(paths)\n",
    "\n",
    "        # Check that number of sentences and tokens per tag are correct\n",
    "        # Also, update the vocabulary for future processing\n",
    "        tags_counter = Counter()\n",
    "        cont_sentences = 0\n",
    "        for sentences in data.values():\n",
    "            cont_sentences += len(sentences)\n",
    "            for sentence in sentences:\n",
    "                for tok, tag in sentence:\n",
    "                    tags_counter.update([tag])\n",
    "                    self.counter.update([tok])\n",
    "\n",
    "        # Number of sentences is correct\n",
    "        assert cont_sentences == 9520 + 1341\n",
    "        # Document types are correct\n",
    "        assert list(data.keys()) == doc_type_GUM\n",
    "        # Number of tokens per tag is correct\n",
    "        assert [(x[0], x[-3]) for x in PoS_tags] == sorted(tags_counter.items())\n",
    "        return data\n",
    "\n",
    "    def read_test_data(self, is_GUM: bool) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "\n",
    "        # Both GUM and GENTLE test datasets will be used to evaluate the model\n",
    "        path = (\n",
    "            os.path.join(self.path_data, \"en_gum-ud-test.conllu\")\n",
    "            if is_GUM\n",
    "            else os.path.join(self.path_data, \"en_gentle-ud-test.conllu\")\n",
    "        )\n",
    "\n",
    "        data = self.read_data([path])\n",
    "\n",
    "        # Check that number of sentences and tokens per tag are correct\n",
    "        cont_sentences = 0\n",
    "        tags_counter = Counter()\n",
    "\n",
    "        for sentences in data.values():\n",
    "            cont_sentences += len(sentences)\n",
    "            for sentence in sentences:\n",
    "                for _, tag in sentence:\n",
    "                    tags_counter.update([tag])\n",
    "\n",
    "        if is_GUM:\n",
    "            # Number of sentences is correct\n",
    "            assert cont_sentences == 1285\n",
    "            # Document types are correct\n",
    "            assert list(data.keys()) == doc_type_GUM\n",
    "            # Number of tokens per tag is correct\n",
    "            assert [(x[0], x[-2]) for x in PoS_tags] == sorted(tags_counter.items())\n",
    "        else:\n",
    "            # Number of sentences is correct\n",
    "            assert cont_sentences == 1334\n",
    "            # Document types are correct\n",
    "            assert list(data.keys()) == doc_type_GENTLE\n",
    "            # Number of tokens per tag is correct\n",
    "            assert [(x[0], x[-1]) for x in PoS_tags] == sorted(tags_counter.items())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def read_data(self, paths: list[str]) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "        data = {}\n",
    "        for path in paths:\n",
    "            assert os.path.exists(path), f\"The {path} path does not exist\"\n",
    "            # Name of the read last document type\n",
    "            last_doc_id = \"\"\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                for tokenlist in parse_incr(file):\n",
    "                    if \"newdoc id\" in tokenlist.metadata:\n",
    "                        # Get the document type and remove unnecessary additional information\n",
    "                        doc_type = \"_\".join(\n",
    "                            tokenlist.metadata[\"newdoc id\"].split(\"_\")[:2]\n",
    "                        )\n",
    "                        # Avoid the first case (\"\") and change of document type if new is found\n",
    "                        if doc_type != last_doc_id:\n",
    "                            last_doc_id = doc_type\n",
    "\n",
    "                    # Auxiliar list to store in Tuples the tokens and tags of the sentence\n",
    "                    auxiliar = []\n",
    "                    for token in tokenlist:\n",
    "                        # Token has the following internal structure:\n",
    "                        # token: <class 'conllu.models.Token'> /// dict_keys(['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'])\n",
    "\n",
    "                        # Avoid _ case as previously explained\n",
    "                        if token[\"upos\"] == \"_\":\n",
    "                            continue\n",
    "                        # Possibility to use the lemma or the form of the token\n",
    "                        auxiliar.append(\n",
    "                            (\n",
    "                                token[\"lemma\" if self.lemmatize else \"form\"].lower(),\n",
    "                                token[\"upos\"],\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    # If the document type is already in the dictionary, append the new data, otherwise, create a new key\n",
    "                    if doc_type in data:\n",
    "                        data[doc_type].append(auxiliar)\n",
    "                    else:\n",
    "                        data[doc_type] = [auxiliar]\n",
    "        return data\n",
    "\n",
    "    def create_vocab(self) -> List[str]:\n",
    "        # Get the most common tokens in the train dataset\n",
    "        tokens, times = zip(*self.counter.most_common())\n",
    "        tokens = np.array(tokens)\n",
    "        times = np.array(times)\n",
    "\n",
    "        # Calculate the index of the tokens that are necessary to reach the threshold\n",
    "        cum_times = np.cumsum(times)\n",
    "        total_tokens = cum_times[-1]\n",
    "        idx = np.searchsorted(cum_times, self.threshold * total_tokens)\n",
    "\n",
    "        return tokens[: idx + 1].tolist()\n",
    "\n",
    "    def train_model(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        emission = pd.DataFrame(\n",
    "            np.zeros((len(self.tags), len(self.vocab)), dtype=np.float64),\n",
    "            columns=self.vocab,\n",
    "            index=self.tags,\n",
    "        )\n",
    "        transition = pd.DataFrame(\n",
    "            np.zeros((len(self.tags), len(self.tags)), dtype=np.float64),\n",
    "            columns=self.tags,\n",
    "            index=self.tags,\n",
    "        )\n",
    "\n",
    "        for sentences in self.train.values():\n",
    "            previous_tag = \"START\"\n",
    "            for sentence in sentences:\n",
    "                for tok, tag in sentence:\n",
    "                    if tok not in self.vocab:\n",
    "                        tok = \"UNK\"\n",
    "                    emission.loc[tag, tok] += 1\n",
    "                    transition.loc[previous_tag, tag] += 1\n",
    "                    previous_tag = tag\n",
    "                transition.loc[previous_tag, \"STOP\"] += 1\n",
    "\n",
    "        ###############################################################################\n",
    "        ################################ EMISSION #####################################\n",
    "        ###############################################################################\n",
    "\n",
    "        row_sums = emission.sum(axis=1)\n",
    "        # Convert row_sums to a numpy array otherwise muldimensional indexing error\n",
    "        row_sums = np.array(row_sums)\n",
    "\n",
    "        # Considering alpha = 1 for smoothing\n",
    "        #'''\n",
    "        emission = np.log2(emission + 1) - np.log2(\n",
    "            row_sums[:, np.newaxis] + len(self.vocab)\n",
    "        )\n",
    "        #'''\n",
    "\n",
    "        \"\"\" Check that the sum of the rows is 1 as it is a probability distribution\n",
    "        emission = (emission + 1) / (row_sums[:, np.newaxis] + len(self.vocab))\n",
    "        row_sums_check = emission.sum(axis=1)\n",
    "        assert np.allclose(row_sums_check, 1), f\"Sums of rows must be 1\"\n",
    "        \"\"\"\n",
    "\n",
    "        ###############################################################################\n",
    "        ################################ TRANSITION #################################\n",
    "        ###############################################################################\n",
    "\n",
    "        row_sums = transition.sum(axis=1)\n",
    "        # Convert row_sums to a numpy array otherwise muldimensional indexing error\n",
    "        row_sums = np.array(row_sums)\n",
    "\n",
    "        # Considering alpha = 1 for smoothing\n",
    "        #'''\n",
    "        transition = np.log2(transition + 1) - np.log2(\n",
    "            row_sums[:, np.newaxis] + len(self.tags)\n",
    "        )\n",
    "        #'''\n",
    "\n",
    "        \"\"\" Check that the sum of the rows is 1 as it is a probability distribution\n",
    "        transition = (transition + 1) / (row_sums[:, np.newaxis] + len(self.tags))\n",
    "        row_sums_check = transition.sum(axis=1)\n",
    "        assert np.allclose(row_sums_check, 1), f\"Sums of rows must be 1\"\n",
    "        \"\"\"\n",
    "\n",
    "        ###############################################################################\n",
    "\n",
    "        return emission, transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_REPO_FOLDER = input(\n",
    "    \"Enter the path to the repository folder (must end in HMM_PoS_Tagger): \"\n",
    ")\n",
    "# PATH_TO_DATA_FOLDER = os.path.join(\"/content\", \"data\")\n",
    "\n",
    "PATH_TO_DATA_FOLDER = os.path.join(PATH_TO_REPO_FOLDER, \"data\")\n",
    "assert os.path.exists(\n",
    "    PATH_TO_DATA_FOLDER\n",
    "), f\"The {PATH_TO_DATA_FOLDER} path does not exist\"\n",
    "LEMMATIZE = True\n",
    "THRESHOLD = 0.9\n",
    "\n",
    "tagger = HMM_PoS_tagger(path_data=PATH_TO_DATA_FOLDER, lemmatize=LEMMATIZE, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWy_2FgUnCs9"
   },
   "source": [
    "## Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Viterbi algorithm is a dynamic programming method used to find the most probable sequence of states in a Hidden Markov Model (HMM), given a series of observations. The algorithm determines the most likely sequence of hidden states that generated the observed data.\n",
    "Rather than enumerating all possible tag sequences, it leverages the Markov assumption in the HMM, which states that the probability of each state depends only on the previous one. The algorithm computes the most likely sequence of states by maximizing the joint probability of the observations and the states using a recursive process. This significantly reduces the computational complexity compared to exhaustively exploring all possible sequences.\n",
    "\n",
    "Mathematically, it is represented as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{argmax}_{y \\in Y} \\prod_{i=1}^{n+1} p(y_i | y_{i-1}) \\prod_{i=1}^{n} p(x_i | y_i)\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gD_ptS94YSOE"
   },
   "outputs": [],
   "source": [
    "# Implementation of the viterbi algorithm\n",
    "def viterbi(sentence, init_matrix, transition_matrix, emission_matrix, word_to_index, category_to_index, index_to_category):\n",
    "    sentence.insert(0, \"START\")\n",
    "    #print(sentence)\n",
    "    num_categories = len(init_matrix) # num categories N\n",
    "    num_words = len(sentence) # num words in the sentence T\n",
    "\n",
    "    # viterbi table\n",
    "    viterbi_table = np.full((num_categories, num_words), -np.inf) # inf for logarithms\n",
    "    backpointer_table = np.zeros((num_categories, num_words), dtype=int)\n",
    "\n",
    "\n",
    "    # first step (init)\n",
    "    for category in range(num_categories):\n",
    "        #print(\"CATEGORY: \", index_to_category[category])\n",
    "        #if index_to_category[category] == \"START\": \n",
    "        #    continue\n",
    "        word_index = word_to_index.get(sentence[0], word_to_index[\"UNK\"]) # we use UNK tag if the word is not in the vocab\n",
    "        #result = init_matrix[category] + emission_matrix[category, word_index]\n",
    "        result = init_matrix[category] + transition_matrix[category_to_index[\"START\"], category]\n",
    "\n",
    "        #print(result)\n",
    "        viterbi_table[category, 0] = result # sum because of logs\n",
    "    # print(viterbi_table)\n",
    "        \n",
    "\n",
    "    # recursion step\n",
    "    for t in range (1, num_words): # from the 2nd to T\n",
    "        for category in range(num_categories): # from 1 to N\n",
    "            max_prob = -np.inf\n",
    "            max_prev_category = 0\n",
    "            word_index = word_to_index.get(sentence[t], word_to_index[\"UNK\"])\n",
    "            #print(sentence[t])\n",
    "\n",
    "            for prev_category in range(num_categories):\n",
    "                prev_viterbi = viterbi_table[prev_category, t-1]\n",
    "                transition_score = transition_matrix[prev_category, category]\n",
    "                '''\n",
    "                if word_index is not None:\n",
    "                    emission_score = emission_matrix[category, word_index]\n",
    "                else:\n",
    "                    emission_score = 0\n",
    "                '''\n",
    "                emission_score = emission_matrix[category, word_index]\n",
    "                current_prob = prev_viterbi + transition_score + emission_score\n",
    "\n",
    "                if current_prob > max_prob:\n",
    "                    max_prob = current_prob\n",
    "                    max_prev_category = prev_category\n",
    "\n",
    "            # update tables viterbi and backpointer\n",
    "            viterbi_table[category, t] = max_prob\n",
    "            backpointer_table[category, t] = max_prev_category # a la que apunta \n",
    "\n",
    "    # final probability step\n",
    "    max_final_category = np.argmax(viterbi_table[:, -1]) # all the rows of the last column\n",
    "    max_prob = viterbi_table[max_final_category, -1] # maximum probability is the one in the last column of the max_final category row\n",
    "\n",
    "    # construct optimal sequence\n",
    "    optimal_seq = [max_final_category]\n",
    "    for t in range(num_words-1, 0, -1): # from num_words-1 until 1, backward\n",
    "        #print(t)\n",
    "        back_value = backpointer_table[optimal_seq[0], t]\n",
    "        optimal_seq.insert(0, back_value)\n",
    "\n",
    "    optimal_tags = [index_to_category[idx] for idx in optimal_seq]\n",
    "    \n",
    "    return sentence[1:], optimal_tags[1:], max_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the input sentence is processed by adding a \"START\" token at the beginning to mark the start of the sequence. Then, two tables are initialized: the viterbi_table, which will store the accumulated probabilities of the most probable state sequences for each word, and the backpointer_table, which will store the indices of the previous states that led to those maximum probabilities.\n",
    "\n",
    "The next step is the processing of the first word, the \"START\" token. For each category, the algorithm calculates the probability of the sequence starting in that category, based on the initial probability and the transition probability from the \"START\" state. This probability is stored in the viterbi_table, and the backpointer_table is updated with the state that led to the maximum probability.\n",
    "\n",
    "The algorithm then enters the recursion step for the subsequent words. For each word in the sentence, and for each possible category, the algorithm calculates the probability of that category being the correct one for the current word, considering the accumulated probabilities from the previous states, the transition probabilities between categories, and the emission probabilities of the current word given the category. This probability is calculated as the sum of the probability of the previous state, the transition probability, and the emission probability. The viterbi_table is updated with the maximum probability, and the backpointer_table is adjusted to store the index of the previous state that leads to this maximum probability.\n",
    "\n",
    "Once all words are processed, the algorithm moves to the final step, where it identifies the category with the maximum probability in the last step (the last word in the sentence). This category represents the most probable state at the end of the sequence.\n",
    "\n",
    "Finally, the algorithm performs a traceback of the optimal category sequence using the backpointer_table. Starting from the state with the maximum probability at the final step, it traces backward through the time steps to reconstruct the most probable sequence of states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a small test to ensure the correct functionality of the Viterbi algorithm. A new sentence is input into the algorithm, and the expected output is the most probable sequence of tags, along with the maximum log probability for the predicted sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_matrix = tagger.emission\n",
    "transition_matrix = tagger.transition\n",
    "\n",
    "'''\n",
    "print(\"Transition Matrix:\")\n",
    "print(transition_matrix)\n",
    "print(\"Emission Matrix:\")\n",
    "print(emission_matrix)\n",
    "'''\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(tagger.vocab)}\n",
    "category_to_index = {tag: idx for idx, tag in enumerate(tagger.tags)}\n",
    "index_to_category = {idx: tag for tag, idx in category_to_index.items()}\n",
    "\n",
    "# Sentence to try\n",
    "sentence1 = [\"this\", \"is\", \"a\", \"test\", \"sentence\"]\n",
    "sentence2 = [\"for\", \"another\", \"test\", \"sentence\"]\n",
    "\n",
    "sentence = sentence1\n",
    "\n",
    "\n",
    "# initialize probs\n",
    "num_categories = len(category_to_index)\n",
    "init_matrix = np.full(num_categories, -np.inf)\n",
    "init_matrix[category_to_index[\"START\"]] = 0\n",
    "\n",
    "# viterbi\n",
    "checked_sentence, optimal_tags, max_prob = viterbi(\n",
    "    sentence,\n",
    "    init_matrix,\n",
    "    transition_matrix.values,\n",
    "    emission_matrix.values,\n",
    "    word_to_index,\n",
    "    category_to_index,\n",
    "    index_to_category\n",
    ")\n",
    "\n",
    "print(\"Sentence:\", checked_sentence)\n",
    "print(\"Optimal Tags:\", optimal_tags)\n",
    "print(\"Max log probability:\", max_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS\n",
    "\n",
    "Rather than creating a very specific tool, the intention was to make\n",
    "a model that is able to reliably perform in a wide variety of\n",
    "categories. \n",
    "\n",
    "For this reason, our training corpus is quite big, and includes\n",
    "data from very different sources. The expected result is for\n",
    "our model to have as much accuracy on out-of-domain data as possible,\n",
    "while maintaining a sufficient performance on in-domain data.\n",
    "\n",
    "As we will see below, to test this hypothesis, we have chosen the GENTLE corpus,\n",
    "made specifically of unusual text types including mathematical proofs and dictionary\n",
    "entries, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-ES_gugYtqB"
   },
   "source": [
    "### In-domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tj6D23qY5IU"
   },
   "source": [
    "First of all, we need to define a function that evaluates our model.\n",
    "We will tell this function which of our two test datasets (GUM or GENTLE)\n",
    "to take to evaluate the model against, and a domain of said datasets.\n",
    "\n",
    "The function will then print the precision and recall by tag, as well\n",
    "as the total accuracy and the confusion matrix. The function should also\n",
    "return the total number of mistakes and tokens, so we can more easily operate\n",
    "with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "znH8oPNXYwzq"
   },
   "outputs": [],
   "source": [
    "#This function calculates the precision and recall of our model\n",
    "#for each tag for a given domain, as well as the total accuracy of our model\n",
    "#where Acc = 100 - (100*NumMistakes/NumTokens)\n",
    "def evaluate(isGUM: bool, domain: str, show_data:bool):\n",
    "\n",
    "    #Check if the domain exists for the 2 corpuses we have been working with\n",
    "    if isGUM:\n",
    "        assert domain in doc_type_GUM, f\"The domain {domain} is not in the GUM dataset\"\n",
    "        test = tagger.test_GUM[domain]\n",
    "    else:\n",
    "        assert (\n",
    "            domain in doc_type_GENTLE\n",
    "        ), f\"The domain {domain} is not in the GENTLE dataset\"\n",
    "        test = tagger.test_GENTLE[domain]\n",
    "\n",
    "    #list_gold contains all of the actual tags for each token\n",
    "    #list_pred cointains the tags our model predicts\n",
    "    list_gold, list_pred = [], []\n",
    "    total_mistakes, total_tokens = 0, 0\n",
    "\n",
    "    #Iterate through each of the sentences and then through each of the tokens\n",
    "    #It's important to remember our data is stored in tuples (token, tag)\n",
    "    for idx in range(len(test)):\n",
    "        newSent = [tok for tok, _ in test[idx]]\n",
    "        gold_tags = [tag for _, tag in test[idx]]\n",
    "        total_tokens += len(newSent)\n",
    "        list_gold.extend(gold_tags)\n",
    "\n",
    "        checked_sentence, predicted_tags, _ = viterbi(\n",
    "            newSent,\n",
    "            init_matrix,\n",
    "            transition_matrix.values,\n",
    "            emission_matrix.values,\n",
    "            word_to_index,\n",
    "            category_to_index,\n",
    "            index_to_category,\n",
    "        )\n",
    "        \n",
    "        #Make sure the pre-processing of the data is correct by comparing lengths \n",
    "        assert [\"START\"] + checked_sentence == newSent, f\"The sentence is not the same\"\n",
    "        assert len(gold_tags) == len(\n",
    "            predicted_tags\n",
    "        ), f\"The length of the tokenized sentence and the predicted tags is not the same\"\n",
    "        list_pred.extend(predicted_tags)\n",
    "\n",
    "        total_mistakes += sum(\n",
    "            1 for pred, gold in zip(predicted_tags, gold_tags) if pred != gold\n",
    "        )\n",
    "\n",
    "    #print(\"Total number of mistakes: \"  + str(total_mistakes))\n",
    "    #print(\"% of mistakes: \" + str(100 - (100 * total_mistakes / total_tokens)))\n",
    "\n",
    "    if show_data == True:\n",
    "\n",
    "        #Create and print the classification report using the sklearn library\n",
    "        report = classification_report(list_gold, list_pred, zero_division=0, digits=4)\n",
    "        print(report)\n",
    "\n",
    "\n",
    "        #Create and print the confusion matrix. The confusion matrix looks very pretty (thanks, jon!)\n",
    "        cm = confusion_matrix(list_gold, list_pred, labels=tagger.tags)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=tagger.tags)\n",
    "        fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "        disp.plot(ax=ax)\n",
    "\n",
    "    return total_mistakes, total_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has two parameteres to tune it:\n",
    "\n",
    "The threshold: How rare a word has to be to be tagged as \"UNK\". The higger this number, the less words\n",
    "will be \"UNK\". As the corpus chosen to train this model is quite big (200k tokens), the initial assumption\n",
    "is that this threshold will be quite high, for a lot of different words will naturally occur in train data.\n",
    "As testing for every 1% increment would take way too long and very low numbers are expected to perform worse,\n",
    "a small sample of the most likely to be the best thresholds has been manually chosen.\n",
    "\n",
    "Lemmatization: Whether we are working with words or lemmas. If TRUE, the test data is also lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Keep into account that given the large amounts of data we are working with, executing this cell might take close to 30 minutes\n",
    "\n",
    "thresholds = [0.85, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "lemmatization = [False, True]\n",
    "\n",
    "#max accuracy\n",
    "accmax = 0\n",
    "for threshold in thresholds:\n",
    "    for lemma in lemmatization:\n",
    "\n",
    "        #process the data with the given parameters\n",
    "        tagger = HMM_PoS_tagger(path_data=PATH_TO_DATA_FOLDER, lemmatize=lemma, threshold=threshold)\n",
    "\n",
    "        #variables to send the viterbi\n",
    "        emission_matrix = tagger.emission\n",
    "        transition_matrix = tagger.transition\n",
    "\n",
    "        word_to_index = {word: idx for idx, word in enumerate(tagger.vocab)}\n",
    "        category_to_index = {tag: idx for idx, tag in enumerate(tagger.tags)}\n",
    "        index_to_category = {idx: tag for tag, idx in category_to_index.items()}\n",
    "\n",
    "        # initialize probs\n",
    "        num_categories = len(category_to_index)\n",
    "        init_matrix = np.full(num_categories, -np.inf)\n",
    "        init_matrix[category_to_index[\"START\"]] = 0\n",
    "\n",
    "        #total mistakes and tokens\n",
    "        ttm = 0\n",
    "        ttt = 0\n",
    "\n",
    "        for domain in doc_type_GUM:\n",
    "\n",
    "            #keep count of mistakes and tokens between domains to calculate accuracy\n",
    "            tm, tt = evaluate(isGUM=True, domain=domain, show_data=False)\n",
    "            ttm += tm\n",
    "            ttt += tt\n",
    "\n",
    "            #compare accuracy and save parameters that give the highest\n",
    "\n",
    "        acc = (100 - (100 * (ttm/ttt)))\n",
    "        if (acc > accmax):\n",
    "            accmax = acc\n",
    "            l = lemma\n",
    "            t = threshold\n",
    "            \n",
    "\n",
    "print(l)\n",
    "print(t)\n",
    "print(accmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the best result when lemmatizing and using a threshold of 97% (that is, the 3% of less common lemmas is turned into UNK).\n",
    "This model gives us an overall accuracy of 86.89%, which is quite good if we take into account how different the categories of our corpus were to each other.\n",
    "\n",
    "Let's see how the different categories of GUM perform with this parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the data with the given parameters\n",
    "tagger = HMM_PoS_tagger(path_data=PATH_TO_DATA_FOLDER, lemmatize=True, threshold=0.97)\n",
    "\n",
    "#variables to send the viterbi\n",
    "emission_matrix = tagger.emission\n",
    "transition_matrix = tagger.transition\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(tagger.vocab)}\n",
    "category_to_index = {tag: idx for idx, tag in enumerate(tagger.tags)}\n",
    "index_to_category = {idx: tag for tag, idx in category_to_index.items()}\n",
    "\n",
    "# initialize probs\n",
    "num_categories = len(category_to_index)\n",
    "init_matrix = np.full(num_categories, -np.inf)\n",
    "init_matrix[category_to_index[\"START\"]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for domain in doc_type_GUM:\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"-------------------------------------------{domain}------------------------------------------------\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    mis, tok = evaluate(isGUM=True, domain=domain, show_data=False)\n",
    "    print(\"The accuracy for \" + str(domain) + \" is: \" + str(100-(100 * mis/tok)))\n",
    "    print(\"####################################################################################################\")\n",
    "    print(\"####################################################################################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing categories are Conversations (92.3%) and Vlogs (90.5%).\n",
    "The worst performing categories are News(84,3%) and Biographies(84,5).\n",
    "\n",
    "We could have probably gotten higher numbers for each of these categories had the model\n",
    "been trained specifially for them. Let's now print the specifics of the data for the best\n",
    "and worst perfoming domains, and then see whether this loss of accuracy paid off by checking\n",
    "if our model is able to keep up with the purposely chosen for being hard to work with domains of GENTLE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , _ = evaluate(isGUM=True, domain=\"GUM_conversation\", show_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , _ = evaluate(isGUM=True, domain=\"GUM_news\", show_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8P3JCgoY6G6"
   },
   "source": [
    "For the same parameters we had before, let's see how well the model performs over the GENTLE corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPf5cUelYzN8"
   },
   "outputs": [],
   "source": [
    "\n",
    "for domain in doc_type_GENTLE:\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"-------------------------------------------{domain}------------------------------------------------\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    mis, tok = evaluate(isGUM=False, domain=domain, show_data=False)\n",
    "    print(\"The accuracy for \" + str(domain) + \" is: \" + str(100-(100 * mis/tok)))\n",
    "    print(\"####################################################################################################\")\n",
    "    print(\"####################################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is able to perform reasonably well on very specific domains (it gets closes to 90% accuracy on Esports commentary and Deah threats), however, the more specific a domain is, the worse it performs. By the point it gets to Mathematical proofs, it has a meager 70% accuracy.\n",
    "\n",
    "Once again, lets see all the data from the best performing and worst performing domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , _ = evaluate(isGUM=False, domain=\"GENTLE_threat\", show_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , _ = evaluate(isGUM=False, domain=\"GENTLE_proof\", show_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, data will be retrieved from \"Mastodon\" social media to further analyze the performance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the data with the given parameters\n",
    "tagger = HMM_PoS_tagger(path_data=PATH_TO_DATA_FOLDER, lemmatize=True, threshold=0.97)\n",
    "\n",
    "#variables to send the viterbi\n",
    "emission_matrix = tagger.emission\n",
    "transition_matrix = tagger.transition\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(tagger.vocab)}\n",
    "category_to_index = {tag: idx for idx, tag in enumerate(tagger.tags)}\n",
    "index_to_category = {idx: tag for tag, idx in category_to_index.items()}\n",
    "\n",
    "# initialize probs\n",
    "num_categories = len(category_to_index)\n",
    "init_matrix = np.full(num_categories, -np.inf)\n",
    "init_matrix[category_to_index[\"START\"]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define query parameters\n",
    "query = ['politics']  #Replace with any desired topic\n",
    "limit = 100  #Maximum number of posts per server\n",
    "df_data_not_processed = pd.DataFrame()  #Initialize empty DataFrame\n",
    "\n",
    "#Function to construct Mastodon server queries\n",
    "def server_query(num, q, limit):\n",
    "    servers = [\n",
    "        \"https://mastodon.social\",\n",
    "        \"https://mstdn.social\",\n",
    "        \"https://mastodon.world\",\n",
    "        \"https://mas.to\",\n",
    "        \"https://mastodon.online\"\n",
    "    ]\n",
    "    if 0 <= num < len(servers):\n",
    "        return f\"{servers[num]}/api/v1/timelines/tag/{q}?limit={limit}\"\n",
    "    return None\n",
    "\n",
    "#Collect data from Mastodon\n",
    "idsList = []\n",
    "data = []\n",
    "for q in query:\n",
    "    for i in range(5):  #Loop through servers\n",
    "        query_string = server_query(i, q, limit)\n",
    "        response = requests.get(query_string)\n",
    "        if response.status_code == 200:\n",
    "            for res in response.json():\n",
    "                c = bs(res[\"content\"], features=\"html.parser\")\n",
    "                text = c.getText()\n",
    "                # Filter for unique English-language posts\n",
    "                if res[\"id\"] not in idsList and res[\"language\"] == 'en':\n",
    "                    idsList.append(res[\"id\"])\n",
    "                    json_obj = {\n",
    "                        \"id\": res[\"id\"],\n",
    "                        \"name\": res[\"account\"][\"username\"],\n",
    "                        \"lang\": res[\"language\"],\n",
    "                        \"text\": text\n",
    "                    }\n",
    "                    data.append(json_obj)\n",
    "\n",
    "df1 = pd.DataFrame.from_dict(data)\n",
    "df_data_not_processed = pd.concat([df_data_not_processed, df1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have collected the data, we will process and label it. In our case, we used the NLTK library to label the data with Universal Dependencies. After tokenizing, lemmatizing, and labeling, we save the tags for each token in a list per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Converts Treebank POS tags to WordNet POS tags for more accurate lemmatization.\n",
    "\n",
    "    Parameters:\n",
    "    - treebank_tag: Treebank POS tag.\n",
    "\n",
    "    Returns:\n",
    "    - WordNet-compatible POS tag or None.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def tokenize_lemmatize_and_tag(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text into sentences and words, lemmatizes words, and performs POS tagging with universal tags.\n",
    "\n",
    "    Parameters:\n",
    "    - text: The input text string.\n",
    "\n",
    "    Returns:\n",
    "    - A list of lists where each sublist represents a sentence with tuples (token, lemma, tag).\n",
    "    - A list of lists where each sublist contains only the universal POS tags per sentence.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_lemmatized_sentences = []\n",
    "    pos_tags_per_sentence = []\n",
    "\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)  # Split into words\n",
    "        pos_tags = pos_tag(tokens, tagset='universal')  # Perform POS tagging with universal tags\n",
    "\n",
    "        sentence_data = []\n",
    "        pos_tags_only = []\n",
    "\n",
    "        for token, pos in pos_tags:\n",
    "            # Get lemma\n",
    "            wordnet_pos = get_wordnet_pos(pos)  # Convert universal tag to WordNet\n",
    "            lemma = lemmatizer.lemmatize(token, pos=wordnet_pos) if wordnet_pos else token\n",
    "\n",
    "            # Add token data: (token, lemma, universal tag)\n",
    "            sentence_data.append((token, lemma, pos))\n",
    "            pos_tags_only.append(pos)  # Add only the tag\n",
    "\n",
    "        tokenized_lemmatized_sentences.append(sentence_data)\n",
    "        pos_tags_per_sentence.append(pos_tags_only)\n",
    "\n",
    "    return tokenized_lemmatized_sentences, pos_tags_per_sentence\n",
    "\n",
    "# Apply the function and store results in new columns\n",
    "df_data_not_processed['tokenized_lemmatized_tagged'], df_data_not_processed['pos_tags_per_sentence'] = zip(*df_data_not_processed['text'].apply(tokenize_lemmatize_and_tag))\n",
    "\n",
    "# Display the results\n",
    "print(df_data_not_processed[['text', 'tokenized_lemmatized_tagged', 'pos_tags_per_sentence']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,we have to evaluate the performance of a custom Viterbi algorithm on tokenized and lemmatized Mastodon data. The code below compares the predicted Part-of-Speech (POS) tags from Viterbi with reference tags provided by NLTK:\n",
    "\n",
    "1. **Input Data**:\n",
    "   - `tokenized_data`: Tokenized and lemmatized Mastodon sentences, where each token is represented as a tuple (`token`, `lemma`, `nltk_tag`).\n",
    "   - `nltk_tags`: Reference POS tags for each token in the sentences, as assigned by NLTK.\n",
    "\n",
    "2. **Evaluation Process**:\n",
    "   - For each sentence:\n",
    "     - Extracts lemmas for use in tagging.\n",
    "     - Uses the Viterbi algorithm to predict POS tags for the tokens.\n",
    "     - Compares the predicted tags to the reference (gold) tags.\n",
    "     - Tracks mismatches and total tokens to compute accuracy.\n",
    "\n",
    "3. **Performance Metrics**:\n",
    "   - Computes classification metrics (precision, recall, F1-score) using `classification_report` from `sklearn`.\n",
    "   - Displays a confusion matrix to visualize tag prediction errors.\n",
    "\n",
    "4. **Output**:\n",
    "   - Returns the total number of mistakes and tokens processed.\n",
    "   - Optionally, displays detailed evaluation results if `show_data` is set to `True`.\n",
    "\n",
    "The accuracy is computed as:  \n",
    "$$\n",
    "\\text{Accuracy} = 100 - \\left( 100 \\times \\frac{\\text{Total Mistakes}}{\\text{Total Tokens}} \\right)\n",
    "$$\n",
    "\n",
    "This setup allows for a comprehensive comparison of the Viterbi-based tagging against the NLTK-provided tags, giving insights into the algorithm's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tag(tag):\n",
    "    \"\"\"\n",
    "    Normalize predicted tags to match NLTK's universal tags.\n",
    "    \"\"\"\n",
    "    tag_mapping = {\n",
    "        'ADP': 'PRT',\n",
    "        'PROPN': 'NOUN',  # Assuming all proper nouns are considered as NOUN in NLTK\n",
    "        'PUNCT': '.',  # Normalizing punctuation\n",
    "        'VERB': 'VERB',\n",
    "        'NOUN': 'NOUN',\n",
    "        'ADJ': 'ADJ',\n",
    "        'ADV': 'ADV',\n",
    "        'DET': 'DET',\n",
    "        'PRON': 'PRON',\n",
    "        'NUM': 'NUM',\n",
    "        'X': 'X'\n",
    "    }\n",
    "    return tag_mapping.get(tag, tag)  # Return the mapped tag, or the original if not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mastodon(tokenized_data, nltk_tags, show_data=True):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the Viterbi algorithm using tokenized and lemmatized Mastodon data.\n",
    "\n",
    "    Parameters:\n",
    "    - tokenized_data: List of lists where each sublist contains tuples (token, lemma, nltk_tag).\n",
    "    - nltk_tags: List of lists containing NLTK tags for each token.\n",
    "    - show_data: Whether to display detailed classification reports and confusion matrices.\n",
    "\n",
    "    Returns:\n",
    "    - total_mistakes: Total number of mismatches between predicted and gold tags.\n",
    "    - total_tokens: Total number of tokens evaluated.\n",
    "    \"\"\"\n",
    "    list_gold, list_pred = [], []\n",
    "    total_mistakes, total_tokens = 0, 0\n",
    "\n",
    "    # Iterate through tokenized data and corresponding NLTK tags\n",
    "    for sentence_data, gold_tags in zip(tokenized_data, nltk_tags):\n",
    "        #print(f\"Sentence data: {sentence_data}\")\n",
    "        # Extract lemmatized tokens for prediction\n",
    "        new_sent = [lemma for token, lemma, pos in sentence_data[0]]\n",
    "        #print(f\"Tokens (new_sent): {new_sent}\")\n",
    "        total_tokens += len(new_sent)\n",
    "        list_gold.extend(gold_tags[0])\n",
    "\n",
    "        # Use Viterbi to predict tags\n",
    "        checked_sentence, predicted_tags, _ = viterbi(\n",
    "            new_sent,\n",
    "            init_matrix,\n",
    "            transition_matrix.values,\n",
    "            emission_matrix.values,\n",
    "            word_to_index,\n",
    "            category_to_index,\n",
    "            index_to_category,\n",
    "        )\n",
    "\n",
    "        #print(f\"Predicted tags: {predicted_tags}\")\n",
    "        #print(f\"Predicted tags before normalization: {predicted_tags}\")  # Debug: Check predicted tags\n",
    "\n",
    "        # Normalize predicted tags to match NLTK tags\n",
    "        predicted_tags = [normalize_tag(tag) for tag in predicted_tags]\n",
    "\n",
    "        #print(f\"Predicted tags after normalization: {predicted_tags}\")\n",
    "\n",
    "        # Ensure pre-processing consistency\n",
    "        assert len(gold_tags[0]) == len(predicted_tags), (\n",
    "            f\"The length of the tokenized sentence ({len(new_sent)}) and predicted tags ({len(predicted_tags)}) do not match.\"\n",
    "        )\n",
    "        list_pred.extend(predicted_tags)\n",
    "\n",
    "        # Count mistakes\n",
    "        total_mistakes += sum(\n",
    "            1 for pred, gold in zip(predicted_tags, gold_tags[0]) if pred != gold\n",
    "        )\n",
    "\n",
    "    if show_data:\n",
    "        # Print classification report\n",
    "        report = classification_report(list_gold, list_pred, zero_division=0, digits=4)\n",
    "        print(report)\n",
    "\n",
    "        # Generate and display confusion matrix\n",
    "        cm = confusion_matrix(list_gold, list_pred, labels=list(set(list_gold)))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(list_gold)))\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "        disp.plot(ax=ax)\n",
    "\n",
    "    # Return number of mistakes and total tokens\n",
    "    return total_mistakes, total_tokens\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `mastodon_data` contains tokenized and lemmatized data as [(token, lemma, nltk_tag), ...]\n",
    "# and `nltk_tags` contains corresponding NLTK tags\n",
    "# mastodon_data = [[(\"This\", \"this\", \"DET\"), (\"is\", \"be\", \"VERB\"), ...], ...]\n",
    "# nltk_tags = [[\"DET\", \"VERB\", ...], ...]\n",
    "\n",
    "# Extract processed data from the DataFrame\n",
    "mastodon_data = df_data_not_processed['tokenized_lemmatized_tagged'].tolist()\n",
    "nltk_tags = df_data_not_processed['pos_tags_per_sentence'].tolist()\n",
    "\n",
    "# Evaluate the model\n",
    "total_mistakes, total_tokens = evaluate_mastodon(mastodon_data, nltk_tags, show_data=True)\n",
    "print(f\"Accuracy: {100 - (100 * total_mistakes / total_tokens):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Overall Performance**:\n",
    "   - The model achieved an **overall accuracy of 70.11%**, indicating reasonable performance, though there are areas that clearly need improvement. However, not all classes are being effectively recognized.\n",
    "\n",
    "2. **Class-wise Performance**:\n",
    "   - **Well-predicted Classes**:\n",
    "     - **NOUN**: The model performs well in predicting nouns, with a **recall** of **0.9632** and an **F1-score** of **0.8406**.\n",
    "     - **DET**: Determiners also have strong performance, with a **recall** of **0.8642** and an **F1-score** of **0.9121**.\n",
    "     - **PRON**: Pronouns are well-recognized with an **F1-score** of **0.8197**.\n",
    "\n",
    "   - **Poorly Predicted Classes**:\n",
    "     - **ADP** (Adpositions), **AUX** (Auxiliary verbs), **CCONJ** (Coordinating conjunctions), **PART** (Particles), **STOP** (Stopwords), and **SYM** (Symbols) show very poor performance, with **precision**, **recall**, and **F1-score** all equal to zero, indicating that the model does not detect these categories at all.\n",
    "\n",
    "3. **Impact of Class Imbalance**:\n",
    "   - The model appears to be biased towards the more frequent classes, such as **NOUN** and **DET**, and struggles to recognize less frequent classes, such as **ADP**, **AUX**, and **CCONJ**. This behavior is typical in scenarios where there is **class imbalance** in the training data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
