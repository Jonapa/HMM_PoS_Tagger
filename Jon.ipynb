{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf9ULXkPYWRz"
   },
   "source": [
    "# HMM PoS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqAfMXv6Ycew"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4ff3_FeYfhJ"
   },
   "source": [
    "## HMM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from conllu import parse_incr\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gucorpling.org/gum/\n",
    "\n",
    "https://gucorpling.org/gum/gentle.html\n",
    "\n",
    "['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "igkBhNAXnt23"
   },
   "outputs": [],
   "source": [
    "# Implementation of the HMM model\n",
    "class HMM_PoS_tagger:\n",
    "    def __init__(self, path_data: str, lemmatize: bool, threshold: float):\n",
    "        self.path_data = path_data\n",
    "        self.lemmatize = lemmatize\n",
    "        self.threshold = threshold\n",
    "        self.counter = Counter()  # 12770 unique words in train\n",
    "        # Read data from train and test datasets & check that is correct\n",
    "        self.train = self.read_train_data()\n",
    "        self.test_GUM = self.read_test_data(is_GUM=True)\n",
    "        self.test_GENTLE = self.read_test_data(is_GUM=False)\n",
    "        # Create vocabulary\n",
    "        self.vocab, self.frec = self.create_vocab()\n",
    "\n",
    "    def read_train_data(self) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "        # Train and Dev datasets will be used to train the model\n",
    "        # https://github.com/UniversalDependencies/UD_English-GUM/blob/master/stats.xml\n",
    "        paths = [\n",
    "            os.path.join(self.path_data, \"en_gum-ud-train.conllu\"),\n",
    "            os.path.join(self.path_data, \"en_gum-ud-dev.conllu\"),\n",
    "        ]\n",
    "\n",
    "        data = self.read_data(paths)\n",
    "\n",
    "        tags = set()\n",
    "        cont_sentences = 0\n",
    "        for sentences in data.values():\n",
    "            cont_sentences += len(sentences)\n",
    "            for sentence in sentences:\n",
    "                for word, tag in sentence:\n",
    "                    tags.add(tag)\n",
    "                    self.counter.update([word])\n",
    "\n",
    "        # Check that number of sentences and tags are correct\n",
    "        assert len(tags) == 17\n",
    "        assert cont_sentences == 9520 + 1341\n",
    "        assert list(data.keys()) == [\n",
    "            \"GUM_academic\",\n",
    "            \"GUM_bio\",\n",
    "            \"GUM_conversation\",\n",
    "            \"GUM_court\",\n",
    "            \"GUM_essay\",\n",
    "            \"GUM_fiction\",\n",
    "            \"GUM_interview\",\n",
    "            \"GUM_letter\",\n",
    "            \"GUM_news\",\n",
    "            \"GUM_podcast\",\n",
    "            \"GUM_speech\",\n",
    "            \"GUM_textbook\",\n",
    "            \"GUM_vlog\",\n",
    "            \"GUM_voyage\",\n",
    "            \"GUM_whow\",\n",
    "        ]\n",
    "        return data\n",
    "\n",
    "    def read_test_data(self, is_GUM: bool) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "        # Both GUM and GENTLE test datasets will be used to evaluate the model\n",
    "        path = (\n",
    "            os.path.join(self.path_data, \"en_gum-ud-test.conllu\")\n",
    "            if is_GUM\n",
    "            else os.path.join(self.path_data, \"en_gentle-ud-test.conllu\")\n",
    "        )\n",
    "\n",
    "        data = self.read_data([path])\n",
    "\n",
    "        # Check that number of sentences and tags are correct\n",
    "        cont_sentences = 0\n",
    "        tags = set()\n",
    "        for sentences in data.values():\n",
    "            cont_sentences += len(sentences)\n",
    "            for sentence in sentences:\n",
    "                for _, tag in sentence:\n",
    "                    tags.add(tag)\n",
    "\n",
    "        if is_GUM:\n",
    "            assert len(tags) == 17\n",
    "            assert cont_sentences == 1285\n",
    "            assert list(data.keys()) == [\n",
    "                \"GUM_academic\",\n",
    "                \"GUM_bio\",\n",
    "                \"GUM_conversation\",\n",
    "                \"GUM_court\",\n",
    "                \"GUM_essay\",\n",
    "                \"GUM_fiction\",\n",
    "                \"GUM_interview\",\n",
    "                \"GUM_letter\",\n",
    "                \"GUM_news\",\n",
    "                \"GUM_podcast\",\n",
    "                \"GUM_speech\",\n",
    "                \"GUM_textbook\",\n",
    "                \"GUM_vlog\",\n",
    "                \"GUM_voyage\",\n",
    "                \"GUM_whow\",\n",
    "            ]\n",
    "        else:\n",
    "            # https://github.com/UniversalDependencies/UD_English-GENTLE/blob/master/stats.xml\n",
    "            assert len(tags) == 17\n",
    "            assert cont_sentences == 1334\n",
    "            assert list(data.keys()) == [\n",
    "                \"GENTLE_dictionary\",\n",
    "                \"GENTLE_esports\",\n",
    "                \"GENTLE_legal\",\n",
    "                \"GENTLE_medical\",\n",
    "                \"GENTLE_poetry\",\n",
    "                \"GENTLE_proof\",\n",
    "                \"GENTLE_syllabus\",\n",
    "                \"GENTLE_threat\",\n",
    "            ]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def read_data(self, paths: list[str]) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "        data = {}\n",
    "        for path in paths:\n",
    "            assert os.path.exists(path), f\"The {path} path does not exist\"\n",
    "            # Name of the read last document type\n",
    "            last_doc_id = \"\"\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                for tokenlist in parse_incr(file):\n",
    "                    if \"newdoc id\" in tokenlist.metadata:\n",
    "                        # https://github.com/UniversalDependencies/UD_English-GUM/tree/master/not-to-release/file-lists\n",
    "                        # https://github.com/UniversalDependencies/UD_English-GENTLE/tree/master/not-to-release/sources\n",
    "                        # Get the document type and remove unnecessary additional information\n",
    "                        doc_type = \"_\".join(\n",
    "                            tokenlist.metadata[\"newdoc id\"].split(\"_\")[:2]\n",
    "                        )\n",
    "                        # Avoid the first case and change of document type\n",
    "                        if doc_type != last_doc_id:\n",
    "                            last_doc_id = doc_type\n",
    "\n",
    "                    # Auxiliar list to store in a Tuple the words and tags of the sentence\n",
    "                    auxiliar = []\n",
    "                    for token in tokenlist:\n",
    "                        # Token has the following structure:\n",
    "                        # token: <class 'conllu.models.Token'> /// dict_keys(['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'])\n",
    "                        if token[\"upos\"] == \"_\":\n",
    "                            \"\"\"AVOID THIS CASE, as _ is not a valid PoS tag\n",
    "                            12-13\tworkforce’s\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "                            12\tworkforce\tworkforce\tNOUN\tNN\tNumber=Sing\t14\tnmod:poss\t14:nmod:poss\t_\n",
    "                            13\t’s\t's\tPART\tPOS\t_\t12\tcase\t12:case\tEntity=42)\n",
    "                            \"\"\"\n",
    "                            continue\n",
    "                        # Possibility to use the lemma or the form of the word\n",
    "                        auxiliar.append(\n",
    "                            (\n",
    "                                token[\"lemma\" if self.lemmatize else \"form\"].lower(),\n",
    "                                token[\"upos\"],\n",
    "                            )\n",
    "                        )\n",
    "                    \n",
    "                    # If the document type is already in the dictionary, append the new data else create a new key\n",
    "                    if doc_type in data:\n",
    "                        data[doc_type].append(auxiliar)\n",
    "                    else:\n",
    "                        data[doc_type] = [auxiliar]\n",
    "        return data\n",
    "\n",
    "    def create_vocab(self) -> List[str]:\n",
    "        # Get the most common words in the train dataset\n",
    "        words, times = zip(*self.counter.most_common())\n",
    "        words = np.array(words)\n",
    "        times = np.array(times)\n",
    "\n",
    "        # Calculate the index of the words that are necessary to reach the threshold\n",
    "        cum_times = np.cumsum(times)\n",
    "        total_words = cum_times[-1]\n",
    "        idx = np.searchsorted(cum_times, self.threshold * total_words)\n",
    "\n",
    "        return words[:idx + 1].tolist(), times[:idx + 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_REPO_FOLDER = input(\n",
    "    \"Enter the path to the repository folder (must end in HMM_PoS_Tagger): \"\n",
    ")\n",
    "\n",
    "PATH_TO_DATA_FOLDER = os.path.join(PATH_TO_REPO_FOLDER, \"data\")\n",
    "assert os.path.exists(\n",
    "    PATH_TO_DATA_FOLDER\n",
    "), f\"The {PATH_TO_DATA_FOLDER} path does not exist\"\n",
    "LEMMATIZE = True\n",
    "THRESHOLD = 0.9\n",
    "\n",
    "tagger = HMM_PoS_tagger(path_data=PATH_TO_DATA_FOLDER, lemmatize=LEMMATIZE, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWy_2FgUnCs9"
   },
   "source": [
    "## Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD_ptS94YSOE"
   },
   "outputs": [],
   "source": [
    "# Implementation of the viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1Re2WKhYi5p"
   },
   "source": [
    "##Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrCgYCEXY68u"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWxNZtm1YtMq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-ES_gugYtqB"
   },
   "source": [
    "### In-domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tj6D23qY5IU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znH8oPNXYwzq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_OnXj8zY5fr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6LxP9V3YxE2"
   },
   "source": [
    "### Out-of-domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8P3JCgoY6G6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPf5cUelYzN8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clof0wvJY6Xw"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAdEfMYfYywC"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CUnaErdY3f_"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
