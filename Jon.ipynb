{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf9ULXkPYWRz"
   },
   "source": [
    "# HMM PoS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqAfMXv6Ycew"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Georgetown University Multilayer Corpus (GUM) is an open source multilayer corpus of richly annotated texts from 16 text types.**\n",
    "\n",
    "- https://gucorpling.org/gum/\n",
    "\n",
    "- Zeldes, Amir (2017) \"The GUM Corpus: Creating Multilayer Resources in the Classroom\". Language Resources and Evaluation 51(3), 581–612. https://link.springer.com/article/10.1007/s10579-016-9343-x\n",
    "\n",
    "- https://github.com/UniversalDependencies/UD_English-GUM/blob/master/README.md\n",
    "\n",
    "\n",
    "**A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation (GENTLE) is a manually annotated multilayer corpus following the same design and annotation layers as GUM, but of unusual text types.**\n",
    "\n",
    "- https://gucorpling.org/gum/gentle.html\n",
    "\n",
    "- Aoyama, Tatsuya, Behzad, Shabnam, Gessler, Luke, Levine, Lauren, Lin, Jessica, Liu, Yang Janet, Peng, Siyao, Zhu, Yilun and Zeldes, Amir (2023) \"GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation\". In: Proceedings of the Seventeenth Linguistic Annotation Workshop (LAW-XVII 2023). Toronto, Canada. https://arxiv.org/abs/2306.01966\n",
    "\n",
    "- https://github.com/UniversalDependencies/UD_English-GENTLE/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 17 unique PoS Tags in the datasets. In the following code chunk the number of tokens per tag is shown followed by the most common lemmas. However, GUM's stats file presents some incosistences regarding the number of tokens per tag (as our algorithm recovers correctly these numbers for the GENTLE case, but are some minimal differences with GUM, we believe the stats file of the repo is wrong). \n",
    "\n",
    "It is worth mentioning that exists a _ tag that should be avoided like the following example:\n",
    "\n",
    "- 12-13\tworkforce’s\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "\n",
    "- 12\tworkforce\tworkforce\tNOUN\tNN\tNumber=Sing\t14\tnmod:poss\t14:nmod:poss\t_\n",
    "\n",
    "- 13\t’s\t's\tPART\tPOS\t_\t12\tcase\t12:case\tEntity=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" https://github.com/UniversalDependencies/UD_English-GUM/blob/master/stats.xml\n",
    "<!-- Statistics of universal POS tags. The comments show the most frequent lemmas. -->\n",
    "<tags unique=\"17\">\n",
    "<tag name=\"ADJ\">13961</tag><!-- good, other, first, new, many, great, little, large, more, different -->\n",
    "<tag name=\"ADP\">20170</tag><!-- of, in, to, for, on, with, at, from, by, as -->\n",
    "<tag name=\"ADV\">10103</tag><!-- so, when, just, then, also, how, now, more, here, really -->\n",
    "<tag name=\"AUX\">11355</tag><!-- be, have, do, can, will, would, should, could, may, might -->\n",
    "<tag name=\"CCONJ\">7057</tag><!-- and, or, but, both, &, either, nor, yet, neither, plus -->\n",
    "<tag name=\"DET\">17331</tag><!-- the, a, this, all, that, some, no, any, every, another -->\n",
    "<tag name=\"INTJ\">2023</tag><!-- like, yeah, oh, well, so, um, uh, no, okay, yes -->\n",
    "<tag name=\"NOUN\">35507</tag><!-- person, time, year, day, thing, way, life, city, world, work -->\n",
    "<tag name=\"NUM\">3994</tag><!-- one, two, 1, three, 2, 3, four, 4, five, 10 -->\n",
    "<tag name=\"PART\">5113</tag><!-- to, not, 's -->\n",
    "<tag name=\"PRON\">17819</tag><!-- I, it, you, we, that, they, he, his, this, your -->\n",
    "<tag name=\"PROPN\">12184</tag><!-- State, President, University, America, York, New, Warhol, Figure, American, south -->\n",
    "<tag name=\"PUNCT\">28955</tag><!-- ,, ., '', -, ?, (, ), —, [, : -->\n",
    "<tag name=\"SCONJ\">3393</tag><!-- that, if, as, because, for, of, by, while, in, after -->\n",
    "<tag name=\"SYM\">317</tag><!-- -, /, $, %, +, =, DKK, €, £, § -->\n",
    "<tag name=\"VERB\">22277</tag><!-- have, know, go, make, do, get, say, be, take, think -->\n",
    "<tag name=\"X\">361</tag><!-- _, et, al., de, 1, 1., 2., in, situ, 2 -->\n",
    "</tags>\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" https://github.com/UniversalDependencies/UD_English-GENTLE/blob/master/stats.xml\n",
    "<!-- Statistics of universal POS tags. The comments show the most frequent lemmas. -->\n",
    "<tags unique=\"17\">\n",
    "<tag name=\"ADJ\">1240</tag><!-- next, other, first, old, open, more, good, straight, chronic, right -->\n",
    "<tag name=\"ADP\">1588</tag><!-- of, in, to, for, with, on, from, by, at, as -->\n",
    "<tag name=\"ADV\">729</tag><!-- then, just, so, well, here, also, thus, how, where, now -->\n",
    "<tag name=\"AUX\">753</tag><!-- be, will, have, can, do, would, may, should, could, shall -->\n",
    "<tag name=\"CCONJ\">618</tag><!-- and, or, but, &, either, /, plus, yet, both, neither -->\n",
    "<tag name=\"DET\">1195</tag><!-- the, a, this, all, no, any, that, some, each, every -->\n",
    "<tag name=\"INTJ\">60</tag><!-- fucking, please, ah, well, oh, okay, so, uh, ha, now -->\n",
    "<tag name=\"NOUN\">3783</tag><!-- week, x, T, project, school, S, person, day, time, y -->\n",
    "<tag name=\"NUM\">386</tag><!-- one, 1, 5, 2, two, 4, 3, X, 10, three -->\n",
    "<tag name=\"PART\">349</tag><!-- to, not, 's -->\n",
    "<tag name=\"PRON\">1188</tag><!-- I, you, he, it, we, his, that, my, your, they -->\n",
    "<tag name=\"PROPN\">901</tag><!-- Company, JavaScript, Book, Proposition, Court, English, Week, Career, React, Agreement -->\n",
    "<tag name=\"PUNCT\">2655</tag><!-- ,, ., :, '', (, ), -, ;, —, ! -->\n",
    "<tag name=\"SCONJ\">234</tag><!-- that, if, as, because, in, by, like, of, while, before -->\n",
    "<tag name=\"SYM\">167</tag><!-- ⪯, ∈, =, -, ⋅, /, %, +, $, ≤ -->\n",
    "<tag name=\"VERB\">1653</tag><!-- have, go, do, get, take, see, follow, make, know, let -->\n",
    "<tag name=\"X\">300</tag><!-- 1., 2., 3., 4., 5., 6., 7., 8., 9., 10. -->\n",
    "</tags>\n",
    "\"\"\"\n",
    "\n",
    "PoS_tags = [\n",
    "    (\"ADJ\", 12348, 1621, 1240),\n",
    "    (\"ADP\", 17702, 2481, 1588),\n",
    "    (\"ADV\", 8989, 1115, 729),\n",
    "    (\"AUX\", 10174, 1189, 753),\n",
    "    (\"CCONJ\", 6218, 839, 618),\n",
    "    (\"DET\", 15224, 2111, 1195),\n",
    "    (\"INTJ\", 1859, 164, 60),\n",
    "    (\"NOUN\", 31274, 4240, 3783),\n",
    "    (\"NUM\", 3554, 440, 386),\n",
    "    (\"PART\", 4595, 519, 349),\n",
    "    (\"PRON\", 16077, 1746, 1188),\n",
    "    (\"PROPN\", 10557, 1627, 901),\n",
    "    (\"PUNCT\", 25928, 3027, 2655),\n",
    "    (\"SCONJ\", 3053, 340, 234),\n",
    "    (\"SYM\", 282, 35, 167),\n",
    "    (\"VERB\", 19865, 2479, 1653),\n",
    "    (\"X\", 329, 32, 300),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents come from a wide variety of sources. In order to group them in an efficient way, those documents belonging to same area are gathered together. \n",
    "\n",
    "For instance, GUM_academic_art, GUM_academic_census... will belong to GUM_academic. \n",
    "\n",
    "In the following code block, each distinction is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/UniversalDependencies/UD_English-GUM/tree/master/not-to-release/sources\n",
    "\n",
    "doc_type_GUM = [\n",
    "    \"GUM_academic\",\n",
    "    \"GUM_bio\",\n",
    "    \"GUM_conversation\",\n",
    "    \"GUM_court\",\n",
    "    \"GUM_essay\",\n",
    "    \"GUM_fiction\",\n",
    "    \"GUM_interview\",\n",
    "    \"GUM_letter\",\n",
    "    \"GUM_news\",\n",
    "    \"GUM_podcast\",\n",
    "    \"GUM_speech\",\n",
    "    \"GUM_textbook\",\n",
    "    \"GUM_vlog\",\n",
    "    \"GUM_voyage\",\n",
    "    \"GUM_whow\",\n",
    "]\n",
    "\n",
    "# https://github.com/UniversalDependencies/UD_English-GENTLE/tree/master/not-to-release/sources\n",
    "\n",
    "doc_type_GENTLE = [\n",
    "    \"GENTLE_dictionary\",\n",
    "    \"GENTLE_esports\",\n",
    "    \"GENTLE_legal\",\n",
    "    \"GENTLE_medical\",\n",
    "    \"GENTLE_poetry\",\n",
    "    \"GENTLE_proof\",\n",
    "    \"GENTLE_syllabus\",\n",
    "    \"GENTLE_threat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4ff3_FeYfhJ"
   },
   "source": [
    "## HMM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from conllu import parse_incr\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igkBhNAXnt23"
   },
   "outputs": [],
   "source": [
    "# Implementation of the HMM model\n",
    "class HMM_PoS_tagger:\n",
    "    def __init__(self, path_data: str, lemmatize: bool, threshold: float):\n",
    "        self.path_data = path_data\n",
    "        self.lemmatize = lemmatize\n",
    "        self.threshold = threshold\n",
    "        self.counter = Counter()  # 12770 unique tokens in train, 188028 total tokens\n",
    "        # Read data from train and test datasets & check that is correct\n",
    "        self.train = self.read_train_data()\n",
    "        self.test_GUM = self.read_test_data(is_GUM=True)\n",
    "        self.test_GENTLE = self.read_test_data(is_GUM=False)\n",
    "        # Create vocabulary\n",
    "        self.vocab = self.create_vocab()\n",
    "        # Add UNK token to the vocabulary\n",
    "        self.vocab = [\"UNK\"] + self.vocab\n",
    "        # Add START, STOP and UNK tags to the tags\n",
    "        self.tags = [\"UNK\", \"START\", \"STOP\"] + [x[0] for x in PoS_tags]\n",
    "        # Train the model and get the emission and transmission matrices\n",
    "        self.emission, self.transmission = self.train_model()\n",
    "\n",
    "    def read_train_data(self) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "\n",
    "        # Train and Dev datasets will be used to train the model\n",
    "        paths = [\n",
    "            os.path.join(self.path_data, \"en_gum-ud-train.conllu\"),\n",
    "            os.path.join(self.path_data, \"en_gum-ud-dev.conllu\"),\n",
    "        ]\n",
    "\n",
    "        data = self.read_data(paths)\n",
    "\n",
    "        # Check that number of sentences and tokens per tag are correct\n",
    "        # Also, update the vocabulary for future processing\n",
    "        tags_counter = Counter()\n",
    "        cont_sentences = 0\n",
    "        for sentences in data.values():\n",
    "            cont_sentences += len(sentences)\n",
    "            for sentence in sentences:\n",
    "                for tok, tag in sentence:\n",
    "                    tags_counter.update([tag])\n",
    "                    self.counter.update([tok])\n",
    "\n",
    "        # Number of sentences is correct\n",
    "        assert cont_sentences == 9520 + 1341\n",
    "        # Document types are correct\n",
    "        assert list(data.keys()) == doc_type_GUM\n",
    "        # Number of tokens per tag is correct\n",
    "        assert [(x[0], x[-3]) for x in PoS_tags] == sorted(tags_counter.items())\n",
    "        return data\n",
    "\n",
    "    def read_test_data(self, is_GUM: bool) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "\n",
    "        # Both GUM and GENTLE test datasets will be used to evaluate the model\n",
    "        path = (\n",
    "            os.path.join(self.path_data, \"en_gum-ud-test.conllu\")\n",
    "            if is_GUM\n",
    "            else os.path.join(self.path_data, \"en_gentle-ud-test.conllu\")\n",
    "        )\n",
    "\n",
    "        data = self.read_data([path])\n",
    "\n",
    "        # Check that number of sentences and tokens per tag are correct\n",
    "        cont_sentences = 0\n",
    "        tags_counter = Counter()\n",
    "\n",
    "        for sentences in data.values():\n",
    "            cont_sentences += len(sentences)\n",
    "            for sentence in sentences:\n",
    "                for _, tag in sentence:\n",
    "                    tags_counter.update([tag])\n",
    "\n",
    "        if is_GUM:\n",
    "            # Number of sentences is correct\n",
    "            assert cont_sentences == 1285\n",
    "            # Document types are correct\n",
    "            assert list(data.keys()) == doc_type_GUM\n",
    "            # Number of tokens per tag is correct\n",
    "            assert [(x[0], x[-2]) for x in PoS_tags] == sorted(tags_counter.items())\n",
    "        else:\n",
    "            # Number of sentences is correct\n",
    "            assert cont_sentences == 1334\n",
    "            # Document types are correct\n",
    "            assert list(data.keys()) == doc_type_GENTLE\n",
    "            # Number of tokens per tag is correct\n",
    "            assert [(x[0], x[-1]) for x in PoS_tags] == sorted(tags_counter.items())\n",
    "\n",
    "        return data\n",
    "\n",
    "    def read_data(self, paths: list[str]) -> Dict[str, List[List[Tuple[str, str]]]]:\n",
    "        data = {}\n",
    "        for path in paths:\n",
    "            assert os.path.exists(path), f\"The {path} path does not exist\"\n",
    "            # Name of the read last document type\n",
    "            last_doc_id = \"\"\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                for tokenlist in parse_incr(file):\n",
    "                    if \"newdoc id\" in tokenlist.metadata:\n",
    "                        # Get the document type and remove unnecessary additional information\n",
    "                        doc_type = \"_\".join(\n",
    "                            tokenlist.metadata[\"newdoc id\"].split(\"_\")[:2]\n",
    "                        )\n",
    "                        # Avoid the first case (\"\") and change of document type if new is found\n",
    "                        if doc_type != last_doc_id:\n",
    "                            last_doc_id = doc_type\n",
    "\n",
    "                    # Auxiliar list to store in Tuples the tokens and tags of the sentence\n",
    "                    auxiliar = []\n",
    "                    for token in tokenlist:\n",
    "                        # Token has the following internal structure:\n",
    "                        # token: <class 'conllu.models.Token'> /// dict_keys(['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'])\n",
    "\n",
    "                        # Avoid _ case as previously explained\n",
    "                        if token[\"upos\"] == \"_\":\n",
    "                            continue\n",
    "                        # Possibility to use the lemma or the form of the token\n",
    "                        auxiliar.append(\n",
    "                            (\n",
    "                                token[\"lemma\" if self.lemmatize else \"form\"].lower(),\n",
    "                                token[\"upos\"],\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    # If the document type is already in the dictionary, append the new data, otherwise, create a new key\n",
    "                    if doc_type in data:\n",
    "                        data[doc_type].append(auxiliar)\n",
    "                    else:\n",
    "                        data[doc_type] = [auxiliar]\n",
    "        return data\n",
    "\n",
    "    def create_vocab(self) -> List[str]:\n",
    "        # Get the most common tokens in the train dataset\n",
    "        tokens, times = zip(*self.counter.most_common())\n",
    "        tokens = np.array(tokens)\n",
    "        times = np.array(times)\n",
    "\n",
    "        # Calculate the index of the tokens that are necessary to reach the threshold\n",
    "        cum_times = np.cumsum(times)\n",
    "        total_tokens = cum_times[-1]\n",
    "        idx = np.searchsorted(cum_times, self.threshold * total_tokens)\n",
    "\n",
    "        return tokens[: idx + 1].tolist()\n",
    "\n",
    "    def train_model(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        emission = pd.DataFrame(\n",
    "            np.zeros((len(self.tags), len(self.vocab)), dtype=np.float64),\n",
    "            columns=self.vocab,\n",
    "            index=self.tags,\n",
    "        )\n",
    "        transmission = pd.DataFrame(\n",
    "            np.zeros((len(self.tags), len(self.tags)), dtype=np.float64),\n",
    "            columns=self.tags,\n",
    "            index=self.tags,\n",
    "        )\n",
    "\n",
    "        for sentences in self.train.values():\n",
    "            previous_tag = \"START\"\n",
    "            for sentence in sentences:\n",
    "                for tok, tag in sentence:\n",
    "                    if tok not in self.vocab:\n",
    "                        emission.loc[tag, \"UNK\"] += 1\n",
    "                        transmission.loc[previous_tag, \"UNK\"] += 1\n",
    "                        previous_tag = \"UNK\"\n",
    "                    else:\n",
    "                        emission.loc[tag, tok] += 1\n",
    "                        transmission.loc[previous_tag, tag] += 1\n",
    "                        previous_tag = tag\n",
    "                transmission.loc[previous_tag, \"STOP\"] += 1\n",
    "\n",
    "        ###############################################################################\n",
    "        ################################ EMISSION #####################################\n",
    "        ###############################################################################\n",
    "\n",
    "        row_sums = emission.sum(axis=1)\n",
    "        # Convert row_sums to a numpy array otherwise muldimensional indexing error\n",
    "        row_sums = np.array(row_sums)\n",
    "\n",
    "        # Considering alpha = 1 for smoothing\n",
    "        #'''\n",
    "        emission = np.log2(emission + 1) - np.log2(\n",
    "            row_sums[:, np.newaxis] + len(self.vocab)\n",
    "        )\n",
    "        #'''\n",
    "\n",
    "        \"\"\" Check that the sum of the rows is 1 as it is a probability distribution\n",
    "        emission = (emission + 1) / (row_sums[:, np.newaxis] + len(self.vocab))\n",
    "        row_sums_check = emission.sum(axis=1)\n",
    "        assert np.allclose(row_sums_check, 1), f\"Sums of rows must be 1\"\n",
    "        \"\"\"\n",
    "\n",
    "        ###############################################################################\n",
    "        ################################ TRANSMISSION #################################\n",
    "        ###############################################################################\n",
    "\n",
    "        row_sums = transmission.sum(axis=1)\n",
    "        # Convert row_sums to a numpy array otherwise muldimensional indexing error\n",
    "        row_sums = np.array(row_sums)\n",
    "\n",
    "        # Considering alpha = 1 for smoothing\n",
    "        #'''\n",
    "        transmission = np.log2(transmission + 1) - np.log2(\n",
    "            row_sums[:, np.newaxis] + len(self.tags)\n",
    "        )\n",
    "        #'''\n",
    "\n",
    "        \"\"\" Check that the sum of the rows is 1 as it is a probability distribution\n",
    "        transmission = (transmission + 1) / (row_sums[:, np.newaxis] + len(self.tags))\n",
    "        row_sums_check = transmission.sum(axis=1)\n",
    "        assert np.allclose(row_sums_check, 1), f\"Sums of rows must be 1\"\n",
    "        \"\"\"\n",
    "\n",
    "        ###############################################################################\n",
    "\n",
    "        return emission, transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_REPO_FOLDER = input(\n",
    "    \"Enter the path to the repository folder (must end in HMM_PoS_Tagger): \"\n",
    ")\n",
    "\n",
    "PATH_TO_DATA_FOLDER = os.path.join(PATH_TO_REPO_FOLDER, \"data\")\n",
    "assert os.path.exists(\n",
    "    PATH_TO_DATA_FOLDER\n",
    "), f\"The {PATH_TO_DATA_FOLDER} path does not exist\"\n",
    "LEMMATIZE = True\n",
    "THRESHOLD = 0.9\n",
    "\n",
    "tagger = HMM_PoS_tagger(path_data=PATH_TO_DATA_FOLDER, lemmatize=LEMMATIZE, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagger.train[\"GUM_academic\"][0])\n",
    "print(tagger.test_GUM[\"GUM_academic\"][0])\n",
    "print(tagger.test_GENTLE[\"GENTLE_dictionary\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWy_2FgUnCs9"
   },
   "source": [
    "## Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD_ptS94YSOE"
   },
   "outputs": [],
   "source": [
    "# Implementation of the viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1Re2WKhYi5p"
   },
   "source": [
    "##Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrCgYCEXY68u"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWxNZtm1YtMq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-ES_gugYtqB"
   },
   "source": [
    "### In-domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tj6D23qY5IU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znH8oPNXYwzq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_OnXj8zY5fr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6LxP9V3YxE2"
   },
   "source": [
    "### Out-of-domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8P3JCgoY6G6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPf5cUelYzN8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clof0wvJY6Xw"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAdEfMYfYywC"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CUnaErdY3f_"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
